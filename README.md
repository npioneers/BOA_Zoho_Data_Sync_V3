# Project Bedrock V2 - Production Implementation

## ğŸ¯ Overview

Project Bedrock V2 is a robust, configuration-driven data synchronization pipeline that creates a canonical database from dual sources:

1. **Stage 1 - Bulk Load**: CSV backup files â†’ Canonical Schema
2. **Stage 2 - Incremental Sync**: JSON API files â†’ Canonical Schema

## ğŸ—ï¸ Architecture

### Validated Foundation
This implementation is based on comprehensive validation in `Zoho_Data_Sync/notebooks/1_mapping_workbench.ipynb`, which proved that both CSV and JSON sources can be successfully mapped to an identical canonical schema.

### Key Components

```
src/data_pipeline/
â”œâ”€â”€ mappings/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ bills_mapping_config.py    # Mapping configurations for Bills entity
â”œâ”€â”€ transformer.py                 # BillsTransformer class with validation
â””â”€â”€ __init__.py

run_rebuild.py                     # Main orchestrator script
config.yaml                        # Configuration file
requirements.txt                   # Updated dependencies
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Configure Data Sources
Edit `config.yaml` to point to your data sources:

```yaml
data_sources:
  csv_backup_path: "path/to/your/csv/backup"
  json_api_path: "path/to/your/json/files"
  target_database: "output/database/canonical.db"
```

### 3. Execute Full Rebuild
```bash
python run_rebuild.py --full-rebuild
```

## ğŸ“‹ Usage Examples

### Full Dual-Source Rebuild
```bash
python run_rebuild.py --full-rebuild
```

### Using Custom Configuration
```bash
python run_rebuild.py --full-rebuild --config my_config.yaml
```

### CSV Bulk Load Only
```bash
python run_rebuild.py --csv-only
```

### JSON Incremental Sync Only
```bash
python run_rebuild.py --json-only
```

### Validation Only (No Processing)
```bash
python run_rebuild.py --validate-only
```

## ğŸ›ï¸ Canonical Schema

The Bills entity uses a **flattened schema** combining Bills headers with Line Items:

### Schema Fields (31 total)
- **Bill Headers (20 fields)**: BillID, VendorID, VendorName, BillNumber, etc.
- **Line Items (11 fields)**: LineItemID, ItemName, Quantity, Rate, Amount, etc.

### Design Principles
- âœ… **PascalCase naming** for consistency
- âœ… **Complete denormalization** (Bills + Line Items in one table)
- âœ… **One row per line item** with bill header repeated
- âœ… **Based on Zoho API documentation** structure

## ğŸ”„ Transformation Logic

### CSV Backup â†’ Canonical
- **Input**: PascalCase with spaces (`"Bill ID"`, `"Vendor Name"`)
- **Process**: Direct column mapping + calculated fields
- **Output**: 31-column canonical structure

### JSON API â†’ Canonical  
- **Input**: snake_case with nested line_items (`bill_id`, `vendor_name`)
- **Process**: Flattening + column mapping (creates separate row per line item)
- **Output**: 31-column canonical structure (identical to CSV output)

## ğŸ› ï¸ Configuration

### Main Configuration (`config.yaml`)
```yaml
data_sources:
  csv_backup_path: "data/backup_dump/Nangsel Pioneers_2025-06-22"
  json_api_path: "output/raw_json"
  target_database: "output/database/canonical.db"

processing:
  batch_size: 1000
  validate_transformations: true
  create_backups: true
  show_progress: true

logging:
  level: "INFO"
  file: "logs/rebuild.log"
  console: true
```

### Mapping Configuration (`src/data_pipeline/mappings/bills_mapping_config.py`)
- `CANONICAL_BILLS_COLUMNS`: Target schema definition
- `CSV_COLUMN_MAPPING`: CSV â†’ Canonical mappings
- `JSON_HEADER_MAPPING`: JSON headers â†’ Canonical mappings
- `JSON_LINE_ITEM_MAPPING`: JSON line items â†’ Canonical mappings
- `CANONICAL_FIELD_DEFAULTS`: Default values for missing fields

## ğŸ“Š Database Structure

### Canonical Bills Table
```sql
CREATE TABLE bills_canonical (
    "BillID" TEXT PRIMARY KEY,
    "VendorID" TEXT,
    "VendorName" TEXT,
    -- ... (31 total columns)
    "TaxPercentage" REAL
);
```

## ğŸ§ª Validation & Testing

### Built-in Validation
- **Schema consistency**: Both sources produce identical column structure
- **Data integrity**: Type validation and constraint checking
- **Transformation verification**: Assert statements ensure correct mapping

### Validated Results from PoC
- âœ… CSV â†’ Canonical mapping: **100% success**
- âœ… JSON â†’ Canonical mapping: **100% success**  
- âœ… Schema alignment: **Perfect match**
- âœ… Flattening logic: **Correctly handles nested line_items**

## ğŸ“ˆ Performance & Scalability

### Optimizations
- **Pandas native operations** for fast CSV processing
- **Batch processing** for large datasets
- **Memory-efficient** flattening for JSON
- **SQLite optimizations** for database operations

### Scalability Features
- **Configuration-driven** entity support
- **Modular transformer** architecture
- **Extensible mapping** system
- **Progress tracking** for large datasets

## ğŸ”§ Development & Extension

### Adding New Entities
1. Create mapping config: `src/data_pipeline/mappings/{entity}_mapping_config.py`
2. Add transformer class to `transformer.py`
3. Update orchestrator in `run_rebuild.py`
4. Add entity configuration to `config.yaml`

### Customizing Transformations
- Modify mapping dictionaries in `bills_mapping_config.py`
- Extend transformation logic in `BillsTransformer` class
- Add custom validation rules as needed

## ğŸ“ File Structure Details

### Key Files
- **`run_rebuild.py`**: Main orchestrator with CLI interface
- **`config.yaml`**: Configuration file for all settings
- **`src/data_pipeline/transformer.py`**: Core transformation classes
- **`src/data_pipeline/mappings/bills_mapping_config.py`**: Bills-specific mappings
- **`requirements.txt`**: Python dependencies

### Generated Files
- **`output/database/canonical.db`**: SQLite canonical database
- **`logs/rebuild.log`**: Execution logs
- **`output/database/*.backup_*.db`**: Automatic database backups

## ğŸ¯ Next Steps

### Immediate Enhancements
1. **Add more entities**: Invoices, Customers, Items, etc.
2. **Enhanced error handling**: Specific error recovery strategies
3. **Progress indicators**: Real-time progress bars for large datasets
4. **Data validation**: Business rule validation beyond schema

### Future Features
1. **Incremental updates**: Smart detection of changed records
2. **Conflict resolution**: Handle data conflicts between sources
3. **Data quality metrics**: Comprehensive data quality reporting
4. **API integration**: Direct API fetching alongside file processing

## ğŸ† Success Metrics

This production implementation delivers:
- âœ… **Unified canonical schema** for all data sources
- âœ… **100% validated transformations** based on comprehensive PoC
- âœ… **Configuration-driven flexibility** for different environments
- âœ… **Robust error handling** and comprehensive logging
- âœ… **Production-ready code** with proper separation of concerns
- âœ… **Scalable architecture** ready for additional entities

**Project Bedrock V2 successfully transforms validated PoC logic into a robust, production-ready data synchronization pipeline!** ğŸš€
