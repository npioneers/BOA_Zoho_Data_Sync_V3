"""
Enhanced Import Engine for JSON-to-Database Operations

This module provides enhanced import capabilities for loading JSON data into the database
with comprehensive field mapping, custom field detection, and batch processing support.

Key Features:
- Enhanced JSON-to-database field mapping with maximum coverage
- Custom field detection and dynamic table schema updates  
- Batch processing with progress tracking and error handling
- Comprehensive data transformation and validation
- Conflict resolution and duplicate handling
- Import strategy selection (full replace, incremental, merge)
- Detailed logging and operation tracking

Architecture:
ImportEngine -> DataLoader -> FieldMapper -> Transformer -> DatabaseWriter -> Validator
"""

import logging
import sqlite3
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from datetime import datetime
import pandas as pd

from .config import ConfigurationManager
from .json_db_mappings import get_entity_json_mapping, get_all_entities
from .database import DatabaseHandler

logger = logging.getLogger(__name__)


@dataclass
class ImportResult:
    """Result of an import operation"""
    entity_name: str
    success: bool
    records_processed: int
    records_imported: int
    records_updated: int
    records_skipped: int
    custom_fields_detected: List[str]
    errors: List[str]
    warnings: List[str]
    execution_time_seconds: float
    import_strategy: str


@dataclass
class BatchResult:
    """Result of a batch import operation"""
    batch_number: int
    batch_size: int
    records_processed: int
    records_successful: int
    records_failed: int
    errors: List[str]
    execution_time_seconds: float


class EnhancedImportEngine:
    """
    Enhanced import engine with comprehensive JSON-to-database capabilities
    
    Features:
    - Maximum field mapping coverage using enhanced mappings
    - Custom field detection and handling
    - Batch processing with configurable batch sizes
    - Multiple import strategies (replace, incremental, merge)
    - Comprehensive error handling and logging
    - Data validation and transformation
    """
    
    def __init__(self, 
                 database_path: str, 
                 config: Optional[ConfigurationManager] = None,
                 batch_size: int = 1000,
                 enable_custom_fields: bool = True):
        """
        Initialize the enhanced import engine
        
        Args:
            database_path: Path to the SQLite database
            config: Configuration manager instance
            batch_size: Number of records to process per batch
            enable_custom_fields: Whether to detect and handle custom fields
        """
        self.database_path = database_path
        self.config = config or ConfigurationManager()
        self.batch_size = batch_size
        self.enable_custom_fields = enable_custom_fields
        self.db_handler = DatabaseHandler(database_path)
        
        # Operation tracking
        self.operations_log = []
        self.custom_fields_cache = {}
        
        logger.info(f"Enhanced Import Engine initialized: {database_path}")
        logger.info(f"Batch size: {batch_size}, Custom fields: {enable_custom_fields}")
    
    def import_entity_data(self, 
                          entity_name: str, 
                          source_file: Path, 
                          import_strategy: str = 'full_replace',
                          custom_fields: Optional[List[str]] = None) -> ImportResult:
        """
        Import data for a single entity with enhanced capabilities
        
        Args:
            entity_name: Name of the entity to import
            source_file: Path to the JSON source file
            import_strategy: Import strategy ('full_replace', 'incremental', 'merge')
            custom_fields: List of custom fields to handle
            
        Returns:
            Detailed import result
        """
        start_time = datetime.now()
        logger.info(f"ðŸš€ Starting enhanced import for {entity_name}")
        logger.info(f"ðŸ“ Source: {source_file}")
        logger.info(f"ðŸ”§ Strategy: {import_strategy}")
        
        result = ImportResult(
            entity_name=entity_name,
            success=False,
            records_processed=0,
            records_imported=0,
            records_updated=0,
            records_skipped=0,
            custom_fields_detected=custom_fields or [],
            errors=[],
            warnings=[],
            execution_time_seconds=0,
            import_strategy=import_strategy
        )
        
        try:
            # Step 1: Load and validate JSON data
            logger.info("ðŸ“Š Loading JSON data...")
            json_data = self._load_json_data(source_file, entity_name)
            if not json_data:
                result.errors.append("No valid JSON data found")
                return result
                
            result.records_processed = len(json_data)
            logger.info(f"ðŸ“‹ Loaded {result.records_processed} records")
            
            # Step 2: Get enhanced mapping
            logger.info("ðŸ—ºï¸ Loading enhanced field mappings...")
            entity_mapping = get_entity_json_mapping(entity_name)
            if not entity_mapping:
                result.errors.append(f"No mapping found for entity: {entity_name}")
                return result
            
            logger.info(f"ðŸŽ¯ Mapping loaded: {len(entity_mapping)} field mappings")
            
            # Step 3: Detect custom fields if enabled
            detected_custom_fields = []
            if self.enable_custom_fields:
                logger.info("ðŸ” Detecting custom fields...")
                detected_custom_fields = self._detect_custom_fields(json_data, entity_mapping)
                result.custom_fields_detected = detected_custom_fields
                if detected_custom_fields:
                    logger.info(f"ðŸ†• Detected {len(detected_custom_fields)} custom fields")
            
            # Step 4: Prepare target table
            table_name = entity_name.lower()
            logger.info(f"ðŸ—ï¸ Preparing target table: {table_name}")
            self._prepare_target_table(table_name, entity_mapping, detected_custom_fields, import_strategy)
            
            # Step 5: Transform and import data in batches
            logger.info(f"âš¡ Starting batch import (batch size: {self.batch_size})")
            batch_results = self._import_data_in_batches(
                json_data, 
                table_name, 
                entity_mapping, 
                detected_custom_fields,
                import_strategy
            )
            
            # Step 6: Calculate final results
            result.records_imported = sum(br.records_successful for br in batch_results)
            result.records_skipped = sum(br.records_failed for br in batch_results)
            
            # Collect any errors from batches
            for batch_result in batch_results:
                result.errors.extend(batch_result.errors)
            
            result.success = len(result.errors) == 0 and result.records_imported > 0
            
            logger.info(f"âœ… Import completed: {result.records_imported}/{result.records_processed} successful")
            
        except Exception as e:
            logger.error(f"âŒ Import failed for {entity_name}: {str(e)}")
            result.errors.append(f"Import failed: {str(e)}")
            result.success = False
        
        finally:
            end_time = datetime.now()
            result.execution_time_seconds = (end_time - start_time).total_seconds()
            logger.info(f"â±ï¸ Import duration: {result.execution_time_seconds:.2f} seconds")
        
        return result
    
    def _load_json_data(self, source_file: Path, entity_name: str) -> List[Dict[str, Any]]:
        """Load and validate JSON data from file"""
        try:
            with open(source_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Handle different JSON structures
            if isinstance(data, list):
                return data
            elif isinstance(data, dict):
                # Look for entity key in various formats
                possible_keys = [
                    entity_name.lower(),
                    entity_name,
                    entity_name.upper(),
                    f"{entity_name.lower()}s",  # plural
                    f"{entity_name}s"
                ]
                
                for key in possible_keys:
                    if key in data and isinstance(data[key], list):
                        return data[key]
                
                # If no matching key found, return empty list
                logger.warning(f"No matching entity key found in JSON for {entity_name}")
                return []
            else:
                logger.error(f"Unexpected JSON structure: {type(data)}")
                return []
                
        except Exception as e:
            logger.error(f"Failed to load JSON data: {str(e)}")
            return []
    
    def _detect_custom_fields(self, json_data: List[Dict], entity_mapping: Dict[str, str]) -> List[str]:
        """Detect custom fields not covered by standard mapping"""
        if not json_data:
            return []
        
        # Get all fields from sample records
        all_json_fields = set()
        sample_size = min(10, len(json_data))  # Sample first 10 records
        
        for record in json_data[:sample_size]:
            all_json_fields.update(record.keys())
        
        # Find unmapped fields
        mapped_fields = set(entity_mapping.keys())
        custom_fields = all_json_fields - mapped_fields
        
        # Filter out common non-data fields
        excluded_patterns = {'_', 'metadata', 'system', 'internal'}
        custom_fields = [
            field for field in custom_fields 
            if not any(pattern in field.lower() for pattern in excluded_patterns)
        ]
        
        return list(custom_fields)
    
    def _prepare_target_table(self, 
                             table_name: str, 
                             entity_mapping: Dict[str, str], 
                             custom_fields: List[str],
                             import_strategy: str):
        """Prepare the target database table"""
        try:
            with sqlite3.connect(self.database_path) as conn:
                cursor = conn.cursor()
                
                # Check if table exists
                cursor.execute("""
                    SELECT name FROM sqlite_master 
                    WHERE type='table' AND name=?
                """, (table_name,))
                
                table_exists = cursor.fetchone() is not None
                
                if import_strategy == 'full_replace':
                    if table_exists:
                        logger.info(f"ðŸ—‘ï¸ Dropping existing table: {table_name}")
                        cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
                    
                    # Create table with enhanced schema
                    self._create_enhanced_table(cursor, table_name, entity_mapping, custom_fields)
                    
                elif not table_exists:
                    # Create table for incremental/merge strategies
                    self._create_enhanced_table(cursor, table_name, entity_mapping, custom_fields)
                
                else:
                    # Table exists and we're doing incremental/merge
                    # Add any new custom fields as columns
                    self._add_custom_field_columns(cursor, table_name, custom_fields)
                
                conn.commit()
                logger.info(f"âœ… Table prepared: {table_name}")
                
        except Exception as e:
            logger.error(f"âŒ Failed to prepare table {table_name}: {str(e)}")
            raise
    
    def _create_enhanced_table(self, 
                              cursor: sqlite3.Cursor, 
                              table_name: str, 
                              entity_mapping: Dict[str, str],
                              custom_fields: List[str]):
        """Create table with enhanced schema including custom fields"""
        
        # Base columns from mapping
        columns = []
        for json_field, db_field in entity_mapping.items():
            # Simple type inference - can be enhanced
            column_type = "TEXT"
            if any(keyword in json_field.lower() for keyword in ['id', 'count', 'number', 'amount']):
                column_type = "TEXT"  # Keep as TEXT for flexibility
            
            columns.append(f"{db_field} {column_type}")
        
        # Add custom field columns
        for custom_field in custom_fields:
            # Convert to safe column name
            safe_name = self._make_safe_column_name(custom_field)
            columns.append(f"{safe_name} TEXT")
        
        # Add standard tracking columns
        columns.extend([
            "created_at TEXT",
            "updated_at TEXT",
            "import_batch_id TEXT"
        ])
        
        create_sql = f"""
            CREATE TABLE {table_name} (
                {', '.join(columns)}
            )
        """
        
        cursor.execute(create_sql)
        logger.info(f"ðŸ“‹ Created table {table_name} with {len(columns)} columns")
    
    def _add_custom_field_columns(self, cursor: sqlite3.Cursor, table_name: str, custom_fields: List[str]):
        """Add custom field columns to existing table"""
        for custom_field in custom_fields:
            safe_name = self._make_safe_column_name(custom_field)
            try:
                cursor.execute(f"ALTER TABLE {table_name} ADD COLUMN {safe_name} TEXT")
                logger.info(f"âž• Added column {safe_name} to {table_name}")
            except sqlite3.OperationalError as e:
                if "duplicate column name" in str(e).lower():
                    logger.debug(f"Column {safe_name} already exists")
                else:
                    logger.warning(f"Failed to add column {safe_name}: {str(e)}")
    
    def _make_safe_column_name(self, field_name: str) -> str:
        """Convert field name to safe database column name"""
        # Replace special characters with underscores
        safe_name = ''.join(c if c.isalnum() else '_' for c in field_name)
        
        # Ensure it starts with letter or underscore
        if safe_name and safe_name[0].isdigit():
            safe_name = f"field_{safe_name}"
        
        # Limit length
        return safe_name[:64].lower()
    
    def _import_data_in_batches(self, 
                               json_data: List[Dict], 
                               table_name: str, 
                               entity_mapping: Dict[str, str],
                               custom_fields: List[str],
                               import_strategy: str) -> List[BatchResult]:
        """Import data in batches with progress tracking"""
        batch_results = []
        total_batches = (len(json_data) + self.batch_size - 1) // self.batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * self.batch_size
            end_idx = min(start_idx + self.batch_size, len(json_data))
            batch_data = json_data[start_idx:end_idx]
            
            logger.info(f"ðŸ“¦ Processing batch {batch_num + 1}/{total_batches} ({len(batch_data)} records)")
            
            batch_result = self._process_batch(
                batch_data, 
                table_name, 
                entity_mapping, 
                custom_fields,
                batch_num + 1
            )
            
            batch_results.append(batch_result)
            
            logger.info(f"âœ… Batch {batch_num + 1} completed: {batch_result.records_successful}/{batch_result.records_processed} successful")
        
        return batch_results
    
    def _process_batch(self, 
                      batch_data: List[Dict], 
                      table_name: str, 
                      entity_mapping: Dict[str, str],
                      custom_fields: List[str],
                      batch_number: int) -> BatchResult:
        """Process a single batch of records"""
        start_time = datetime.now()
        
        result = BatchResult(
            batch_number=batch_number,
            batch_size=len(batch_data),
            records_processed=len(batch_data),
            records_successful=0,
            records_failed=0,
            errors=[],
            execution_time_seconds=0
        )
        
        try:
            with sqlite3.connect(self.database_path) as conn:
                cursor = conn.cursor()
                
                for record in batch_data:
                    try:
                        # Transform record using enhanced mapping
                        transformed_record = self._transform_record(record, entity_mapping, custom_fields)
                        
                        # Insert record
                        self._insert_record(cursor, table_name, transformed_record, batch_number)
                        result.records_successful += 1
                        
                    except Exception as e:
                        result.records_failed += 1
                        result.errors.append(f"Record failed: {str(e)}")
                        logger.debug(f"Failed to process record: {str(e)}")
                
                conn.commit()
                
        except Exception as e:
            result.errors.append(f"Batch processing failed: {str(e)}")
            logger.error(f"âŒ Batch {batch_number} failed: {str(e)}")
        
        finally:
            end_time = datetime.now()
            result.execution_time_seconds = (end_time - start_time).total_seconds()
        
        return result
    
    def _transform_record(self, 
                         json_record: Dict[str, Any], 
                         entity_mapping: Dict[str, str],
                         custom_fields: List[str]) -> Dict[str, Any]:
        """Transform JSON record to database record using enhanced mapping"""
        transformed = {}
        
        # Apply standard mapping
        for json_field, db_field in entity_mapping.items():
            if json_field in json_record:
                value = json_record[json_field]
                # Apply any necessary transformations
                transformed[db_field] = self._transform_field_value(value)
        
        # Add custom fields
        for custom_field in custom_fields:
            if custom_field in json_record:
                safe_name = self._make_safe_column_name(custom_field)
                value = json_record[custom_field]
                transformed[safe_name] = self._transform_field_value(value)
        
        # Add tracking fields
        now = datetime.now().isoformat()
        transformed.update({
            'created_at': now,
            'updated_at': now,
            'import_batch_id': f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        })
        
        return transformed
    
    def _transform_field_value(self, value: Any) -> str:
        """Transform field value for database storage"""
        if value is None:
            return None
        elif isinstance(value, (dict, list)):
            # Convert complex types to JSON strings
            return json.dumps(value)
        else:
            # Convert to string
            return str(value)
    
    def _insert_record(self, cursor: sqlite3.Cursor, table_name: str, record: Dict[str, Any], batch_number: int):
        """Insert a single record into the database"""
        columns = list(record.keys())
        values = list(record.values())
        placeholders = ', '.join(['?' for _ in values])
        
        sql = f"""
            INSERT INTO {table_name} ({', '.join(columns)})
            VALUES ({placeholders})
        """
        
        cursor.execute(sql, values)
    
    def get_import_statistics(self) -> Dict[str, Any]:
        """Get comprehensive import statistics"""
        return {
            'operations_count': len(self.operations_log),
            'custom_fields_cache': dict(self.custom_fields_cache),
            'batch_size': self.batch_size,
            'custom_fields_enabled': self.enable_custom_fields
        }
